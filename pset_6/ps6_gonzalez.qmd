---
title: "Problem Set 6"
author: "Nicholas R. Gonzalez"
format: pdf
editor: visual
---

Worked on with Tanner Bentley. 

# Problem 1

## Hints

```{r}
#install.packages("mlmRev")
library(mlmRev)
df <- Hsb82

View(Hsb82)

df$school <- as.factor(df$school)

unique(df$sector)

df$sector <- relevel(factor(df$sector), ref = "Public")

levels(df$sector)



```

### a

Regarding school type, attending a catholic school is associated with a 2.25 increase in math achievement, suggesting a positive effect for Catholic schools and math skills 

Regarding minority status, being a racial minority is associated with a 3.11 lower math score compared to whites. This highlights a drastic gap in peformance in math between whites, and non whites. 

According to our data, females score 1.42 points lower than male students, suggesting a gender gap, but not a drastic one. 

Higher socio-economic status is associated with an increase in math scores, by 2.364 points. 

In short, being from a catholic school, suggest higher math performance, as well as being a white, affluent male. 


```{r}
model <- lm(mAch ~ sector + minrty + sx + ses, data = df)

library(modelsummary)

modelsummary(model)

```

### b

For our data, clustering the standard errors, or clustering in general is appropriate because each case has many students that go to one school. So observations are not independent. 


### c

The coefficient results stayed the same, but my standard errors increased by about 100%. This is because without clustering the standard errors assume each student is independent, therefore clustering helps us adjust for intra-school correlaiton 


```{r}
library(estimatr)

df$school_yr <- as.numeric(factor(df$school))

model_clustered <- lm_robust(mAch ~ sector + minrty + sx + ses, 
                             data = df, 
                             clusters = school_yr)  

modelsummary(model_clustered)
```

### d

All results are within a hundredth of each other, and the coefficients remain the same. 

```{r}
#install.packages('sandwich')
#install.packages('lmtest')
library(sandwich)
library(lmtest)

model <- lm(mAch ~ sector + minrty + sx + ses, data = df)

summary(model)

vcov_clustered <- vcovCL(model, cluster = ~ school_yr)

coeftest(model, vcov = vcov_clustered) 

```

### e

After the bootstrap, my standard errors increased. This is likely because the bootstrap approach led to larger variance. It does so by re-sampling the data. This may mean that effects were not well estimated before. We have a decent size sample however, so I am not sure that bootstrapping is necessary. 


```{r}
library(multiwayvcov)
library(lmtest)

model <- lm(mAch ~ sector + minrty + sx + ses, data = df)
summary(model)

set.seed(123) 

vcov_boot <- cluster.boot(model, cluster = df$school_yr, R = 1000, parallel = TRUE)

coeftest(model, vcov = vcov_boot)

```

# Problem 2

### a

```{r}
pset6data <- function(m, n){ 
  beta0 <- 0.3
  beta1 <- 0.8
  cluster.id <- sort(rep(1:m,n))
  d <- as.data.frame(cluster.id)
  d$X <- c()
  d$Y <- c()
  d$v <- c()
  d$epsilon <- c()
  for(i in 1:m){ # For each group
    v <- rnorm(1, 0, 0.5) # Group component in U
    mu <- rnorm(1, 0, 0.5) # Group component in X
    for(j in 1:n){ # For each observation in the group
      d$v[(i-1)*n+j] <- v
      d$X[(i-1)*n+j] <- rnorm(1, 0, 1) + mu
      d$epsilon[(i-1)*n+j] <- rnorm(1, 0, 0.5) # Individual component in U
      d$Y[(i-1)*n+j] <- beta0 + beta1*d$X[(i-1)*n+j] + d$v[(i-1)*n+j] + d$epsilon[(i-1)*n+j]
    }
  }
  return(d)
}
```

```{r}

set.seed(123)

scenario_one <- pset6data(10, 500)

model_one <- lm(Y ~ X, data = scenario_one)

model_one_cluster <- lm_robust(Y ~ X, data = scenario_one, clusters = scenario_one$cluster.id)

scenario_two <- pset6data(50, 100)

model_two <- lm(Y ~ X, data = scenario_two)

model_two_cluster <- lm_robust(Y ~ X, data = scenario_two, clusters = scenario_one$cluster.id)

scenario_three <- pset6data(100, 50)

model_three <- lm(Y ~ X, data = scenario_three)

model_three_cluster <- lm_robust(Y ~ X, data = scenario_three, clusters = scenario_one$cluster.id)

scenario_four <- pset6data(500, 10)

model_four <- lm(Y ~ X, data = scenario_four)

model_four_cluster <- lm_robust(Y ~ X, data = scenario_four, clusters = scenario_one$cluster.id)

# cant do model summary with lm_robust?

summary(model_one)
summary(model_one_cluster)
summary(model_two)
summary(model_two_cluster)
summary(model_three)
summary(model_three_cluster)
summary(model_four)
summary(model_four_cluster)






```

### b


For our first scenario, the coefficients remain the same in both models, but the standard errors in the non-clustered model(0.0085) is smaller than in the clustered model (0.0311). This is as expected, from what we previously discussed about clustering. 

This is also true for scenario two, where standard error in the non-clustered model (0.0086) is again smaller than the clustered model (0.0379). 

This same logic upholds across every scenario, but with their respective results. 

For scenarios three and four, again the coefficients are unchanged. However the ratio changes. The ratios are S1: 3.65, S2: 4.11, S3: 1.54, S4: 1. 58. This is because the degree of clustering, or the amount of grouping differs with each scenario. Also the ratio like changes based on the intra-cluster effects. Which it makes sense as scenarios three and four have more clusters, but less units. 

Is it be safe to say that larger ratios mean stronger intra-cluster effects wheresas smaller eans weaker? 

