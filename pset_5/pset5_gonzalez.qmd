---
title: "pset5_gonzalez"
author: "Nicholas R. Gonzalez"
format: pdf
editor: visual
---

# Problem 1

## a

A _Type II_ error in this hypothesis test would be if we failed to reject the null hypothesis, despite the alternative hypothesis being true. This would mean that the test's results are incorrect, and there is no real effect that exists. 


## b

### i

```{r}

effect <- seq(0.1, 3, by = 0.1)

variance <- seq(1, 10, by = 1)

sample <- seq(10, 200, by = 10)

effect
variance
sample

```

## ii



```{r}

library(tidyverse)
library(stats)

results <- data.frame(effect = numeric(0), var = numeric(0), sample = numeric(0), power = numeric(0))

for (e in effect) {
  for (v in variance) {
    for (n in sample) {
      power_result <- power.t.test(n = n, delta = e, sd = sqrt(v), sig.level = 0.05, type = "two.sample")
      
      results <- rbind(results, data.frame(effect = e, variance = v, sample = n, power = power_result$power))
    }
  }
}

effect_plot <- ggplot(subset(results, variance == 5 & sample == 50), aes(x = effect, y = power)) +
  geom_line(color = "violet", size = 1.5) +
  labs(title = "Power vs. Effect Size", x = "Effect Size", y = "Power") +
  theme_minimal()

variance_plot <- ggplot(subset(results, effect == 0.5 & sample == 50), aes(x = variance, y = power)) +
  geom_line( color = "blue", size = 1.5) +
  labs(title = "Power vs. Variance", x = "Variance", y = "Power") +
  theme_minimal()

sample_plot <- ggplot(subset(results, effect == 0.5 & variance == 5), aes(x = sample, y = power)) +
  geom_line(color = "green", size = 1.5) +
  labs(title = "Power vs. Sample Size", x = "Sample Size", y = "Power") +
  theme_minimal()

print(effect_plot)
print(variance_plot)
print(sample_plot)

```


### iii

As the sample size increases, the power increases. This is a linear increase.

As the effect size increases, power also increases. Something with a big effect would be easier to detect. Our graph shows that once we get to a certain power as well, the effect flat lines. 

As variance increases, power decreases. This is because are data, or results become less interprettable with the more variance we have. 


## c


### i

```{r}

set.seed(1)

reject <- numeric(2000)

for (i in 1:2000) {
  X <- rnorm(n = 20, mean = 5, sd = sqrt(0.1))
  u <- rnorm(n = 20, mean = 0, sd = sqrt(1))
  Y <- 10 + 5*X + u 
  mod <- lm(Y~X)
  p_value <- summary(mod)$coefficients[2,4]
  reject[i] <- p_value < 0.05
}

mean(reject)

```

### ii

The increase in our sample now gives our model the ability to reject 100% of the null hypothesis, as opposed to .995. The reason for this is the bigger the sample within the model, the increased likelihood of us getting the true effect. In general, tests become more precise with more data. 


```{r}

set.seed(123)


for (i in 1:2000) {
  x <- rnorm(n = 100, mean = 5, sd = sqrt(0.1))
  u <- rnorm(n = 100, mean = 0, sd = sqrt(1))
  y <- 10 + 5 * x + u 
  mod <- lm(y~x)
  p_value <- summary(mod)$coefficients[2,4]
  reject[i] <- p_value < 0.05
}

mean(reject)

```


```{r}

```

